{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION\n",
    "\n",
    "### WHAT IT IS?\n",
    "Linear regression is one of the most basic and simple Machine Learning algorithms. It is a supervised learning algorithm, which means that the training data consists of a set of attributes and each one contains a label, the desired output.\n",
    "\n",
    "This method solves a regression problem because the output distribution is continuous.\n",
    "\n",
    "The objective is to build an algorithm that given a vector $x \\in \\mathbb{R}^{n}$ as the input can predict the value of a scalar $y \\in \\mathbb{R}$. The output $y$ is a linear function of the input and there the name.\n",
    "Let $\\hat{y}$ the value that our model predicts y should take, we define the output to be $\\hat{y} = w^\\top x$, where $w \\in \\mathbb{R}^n$ is the vector of parameters we have to learn.\n",
    "\n",
    "###  General explanation of notebook\n",
    "The structure of the notebook is getting all the data and process it to standirize it. Later, we will use the scikit-learn library implementation of Linear Regression. Finally, we will go through the mathematical explanation and implement it from scratch to compare it.\n",
    "\n",
    "### 1. Getting the data\n",
    "First of all, we have to import all of the dependencies that we will use to retrieve the data and cleaning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non estandarized data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>normalized-losses</th>\n",
       "      <th>make</th>\n",
       "      <th>fuel-type</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>num-of-doors</th>\n",
       "      <th>body-style</th>\n",
       "      <th>drive-wheels</th>\n",
       "      <th>engine-location</th>\n",
       "      <th>wheel-base</th>\n",
       "      <th>...</th>\n",
       "      <th>engine-size</th>\n",
       "      <th>fuel-system</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression-ratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peak-rpm</th>\n",
       "      <th>city-mpg</th>\n",
       "      <th>highway-mpg</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>convertible</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>88.6</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>13495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>convertible</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>88.6</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>16500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>hatchback</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>94.5</td>\n",
       "      <td>...</td>\n",
       "      <td>152</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.47</td>\n",
       "      <td>9.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>16500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>164.0</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.8</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>164.0</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4wd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.4</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   symboling  normalized-losses         make fuel-type aspiration  \\\n",
       "0          3                NaN  alfa-romero       gas        std   \n",
       "1          3                NaN  alfa-romero       gas        std   \n",
       "2          1                NaN  alfa-romero       gas        std   \n",
       "3          2              164.0         audi       gas        std   \n",
       "4          2              164.0         audi       gas        std   \n",
       "\n",
       "  num-of-doors   body-style drive-wheels engine-location  wheel-base   ...     \\\n",
       "0          two  convertible          rwd           front        88.6   ...      \n",
       "1          two  convertible          rwd           front        88.6   ...      \n",
       "2          two    hatchback          rwd           front        94.5   ...      \n",
       "3         four        sedan          fwd           front        99.8   ...      \n",
       "4         four        sedan          4wd           front        99.4   ...      \n",
       "\n",
       "   engine-size  fuel-system  bore  stroke compression-ratio horsepower  \\\n",
       "0          130         mpfi  3.47    2.68               9.0      111.0   \n",
       "1          130         mpfi  3.47    2.68               9.0      111.0   \n",
       "2          152         mpfi  2.68    3.47               9.0      154.0   \n",
       "3          109         mpfi  3.19    3.40              10.0      102.0   \n",
       "4          136         mpfi  3.19    3.40               8.0      115.0   \n",
       "\n",
       "   peak-rpm city-mpg  highway-mpg    price  \n",
       "0    5000.0       21           27  13495.0  \n",
       "1    5000.0       21           27  16500.0  \n",
       "2    5000.0       19           26  16500.0  \n",
       "3    5500.0       24           30  13950.0  \n",
       "4    5500.0       18           22  17450.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"../datasets/car/imports-85.data\"\n",
    "\n",
    "# load dataset into Pandas DataFrame\n",
    "df = pd.read_csv(url, na_values= ['?'])\n",
    "features = df.columns\n",
    "\n",
    "print(\"Non estandarized data\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original dataset, containing NaN values as well as non-numerical attributes. Since this model uses a linear function of the attributes, we must convert them to the appropiate type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>normalized-losses</th>\n",
       "      <th>make</th>\n",
       "      <th>fuel-type</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>num-of-doors</th>\n",
       "      <th>body-style</th>\n",
       "      <th>drive-wheels</th>\n",
       "      <th>engine-location</th>\n",
       "      <th>wheel-base</th>\n",
       "      <th>...</th>\n",
       "      <th>engine-size</th>\n",
       "      <th>fuel-system</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression-ratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peak-rpm</th>\n",
       "      <th>city-mpg</th>\n",
       "      <th>highway-mpg</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>88.6</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>5</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>13495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>88.6</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>5</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>16500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>94.5</td>\n",
       "      <td>...</td>\n",
       "      <td>152</td>\n",
       "      <td>5</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.47</td>\n",
       "      <td>9.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>16500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>164.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>99.8</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>5</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>164.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99.4</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>5</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   symboling  normalized-losses  make  fuel-type  aspiration  num-of-doors  \\\n",
       "0          3              122.0     0          1           0             1   \n",
       "1          3              122.0     0          1           0             1   \n",
       "2          1              122.0     0          1           0             1   \n",
       "3          2              164.0     1          1           0             0   \n",
       "4          2              164.0     1          1           0             0   \n",
       "\n",
       "   body-style  drive-wheels  engine-location  wheel-base   ...     \\\n",
       "0           0             2                0        88.6   ...      \n",
       "1           0             2                0        88.6   ...      \n",
       "2           2             2                0        94.5   ...      \n",
       "3           3             1                0        99.8   ...      \n",
       "4           3             0                0        99.4   ...      \n",
       "\n",
       "   engine-size  fuel-system  bore  stroke  compression-ratio  horsepower  \\\n",
       "0          130            5  3.47    2.68                9.0       111.0   \n",
       "1          130            5  3.47    2.68                9.0       111.0   \n",
       "2          152            5  2.68    3.47                9.0       154.0   \n",
       "3          109            5  3.19    3.40               10.0       102.0   \n",
       "4          136            5  3.19    3.40                8.0       115.0   \n",
       "\n",
       "   peak-rpm  city-mpg  highway-mpg    price  \n",
       "0    5000.0        21           27  13495.0  \n",
       "1    5000.0        21           27  16500.0  \n",
       "2    5000.0        19           26  16500.0  \n",
       "3    5500.0        24           30  13950.0  \n",
       "4    5500.0        18           22  17450.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in df.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']):\n",
    "    df[col] = pd.to_numeric(df[col])\n",
    "    df[col] = df[col].replace(np.nan, df[col].mean())\n",
    "\n",
    "for col in df.select_dtypes(exclude=[\"number\"]).columns:\n",
    "    df[col] = df[col].astype('category')    \n",
    "    df[col] = df[col].cat.codes\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that every attributes is a value, we will standarize the data to increase the performance of the algorithm. Tutorials on this topic can be find on the internet, we will use scikit-learn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only want to standarize the input and not the output, which is the price, we have to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>normalized-losses</th>\n",
       "      <th>make</th>\n",
       "      <th>fuel-type</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>num-of-doors</th>\n",
       "      <th>body-style</th>\n",
       "      <th>drive-wheels</th>\n",
       "      <th>engine-location</th>\n",
       "      <th>wheel-base</th>\n",
       "      <th>...</th>\n",
       "      <th>num-of-cylinders</th>\n",
       "      <th>engine-size</th>\n",
       "      <th>fuel-system</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression-ratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peak-rpm</th>\n",
       "      <th>city-mpg</th>\n",
       "      <th>highway-mpg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.743470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.948256</td>\n",
       "      <td>0.328798</td>\n",
       "      <td>-0.469295</td>\n",
       "      <td>1.120713</td>\n",
       "      <td>-3.050975</td>\n",
       "      <td>1.213330</td>\n",
       "      <td>-0.121867</td>\n",
       "      <td>-1.690772</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147475</td>\n",
       "      <td>0.074449</td>\n",
       "      <td>0.869568</td>\n",
       "      <td>0.519089</td>\n",
       "      <td>-1.839404</td>\n",
       "      <td>-0.288349</td>\n",
       "      <td>0.171065</td>\n",
       "      <td>-0.263484</td>\n",
       "      <td>-0.646553</td>\n",
       "      <td>-0.546059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.743470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.948256</td>\n",
       "      <td>0.328798</td>\n",
       "      <td>-0.469295</td>\n",
       "      <td>1.120713</td>\n",
       "      <td>-3.050975</td>\n",
       "      <td>1.213330</td>\n",
       "      <td>-0.121867</td>\n",
       "      <td>-1.690772</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147475</td>\n",
       "      <td>0.074449</td>\n",
       "      <td>0.869568</td>\n",
       "      <td>0.519089</td>\n",
       "      <td>-1.839404</td>\n",
       "      <td>-0.288349</td>\n",
       "      <td>0.171065</td>\n",
       "      <td>-0.263484</td>\n",
       "      <td>-0.646553</td>\n",
       "      <td>-0.546059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.133509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.948256</td>\n",
       "      <td>0.328798</td>\n",
       "      <td>-0.469295</td>\n",
       "      <td>1.120713</td>\n",
       "      <td>-0.717207</td>\n",
       "      <td>1.213330</td>\n",
       "      <td>-0.121867</td>\n",
       "      <td>-0.708596</td>\n",
       "      <td>...</td>\n",
       "      <td>1.112210</td>\n",
       "      <td>0.604046</td>\n",
       "      <td>0.869568</td>\n",
       "      <td>-2.404862</td>\n",
       "      <td>0.685920</td>\n",
       "      <td>-0.288349</td>\n",
       "      <td>1.261807</td>\n",
       "      <td>-0.263484</td>\n",
       "      <td>-0.953012</td>\n",
       "      <td>-0.691627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.938490</td>\n",
       "      <td>1.328961</td>\n",
       "      <td>-1.788499</td>\n",
       "      <td>0.328798</td>\n",
       "      <td>-0.469295</td>\n",
       "      <td>-0.826289</td>\n",
       "      <td>0.449677</td>\n",
       "      <td>-0.589081</td>\n",
       "      <td>-0.121867</td>\n",
       "      <td>0.173698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147475</td>\n",
       "      <td>-0.431076</td>\n",
       "      <td>0.869568</td>\n",
       "      <td>-0.517248</td>\n",
       "      <td>0.462157</td>\n",
       "      <td>-0.035973</td>\n",
       "      <td>-0.057230</td>\n",
       "      <td>0.787346</td>\n",
       "      <td>-0.186865</td>\n",
       "      <td>-0.109354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.938490</td>\n",
       "      <td>1.328961</td>\n",
       "      <td>-1.788499</td>\n",
       "      <td>0.328798</td>\n",
       "      <td>-0.469295</td>\n",
       "      <td>-0.826289</td>\n",
       "      <td>0.449677</td>\n",
       "      <td>-2.391492</td>\n",
       "      <td>-0.121867</td>\n",
       "      <td>0.107110</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.407161</td>\n",
       "      <td>0.218885</td>\n",
       "      <td>0.869568</td>\n",
       "      <td>-0.517248</td>\n",
       "      <td>0.462157</td>\n",
       "      <td>-0.540725</td>\n",
       "      <td>0.272529</td>\n",
       "      <td>0.787346</td>\n",
       "      <td>-1.106241</td>\n",
       "      <td>-1.273900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   symboling  normalized-losses      make  fuel-type  aspiration  \\\n",
       "0   1.743470           0.000000 -1.948256   0.328798   -0.469295   \n",
       "1   1.743470           0.000000 -1.948256   0.328798   -0.469295   \n",
       "2   0.133509           0.000000 -1.948256   0.328798   -0.469295   \n",
       "3   0.938490           1.328961 -1.788499   0.328798   -0.469295   \n",
       "4   0.938490           1.328961 -1.788499   0.328798   -0.469295   \n",
       "\n",
       "   num-of-doors  body-style  drive-wheels  engine-location  wheel-base  \\\n",
       "0      1.120713   -3.050975      1.213330        -0.121867   -1.690772   \n",
       "1      1.120713   -3.050975      1.213330        -0.121867   -1.690772   \n",
       "2      1.120713   -0.717207      1.213330        -0.121867   -0.708596   \n",
       "3     -0.826289    0.449677     -0.589081        -0.121867    0.173698   \n",
       "4     -0.826289    0.449677     -2.391492        -0.121867    0.107110   \n",
       "\n",
       "      ...       num-of-cylinders  engine-size  fuel-system      bore  \\\n",
       "0     ...              -0.147475     0.074449     0.869568  0.519089   \n",
       "1     ...              -0.147475     0.074449     0.869568  0.519089   \n",
       "2     ...               1.112210     0.604046     0.869568 -2.404862   \n",
       "3     ...              -0.147475    -0.431076     0.869568 -0.517248   \n",
       "4     ...              -1.407161     0.218885     0.869568 -0.517248   \n",
       "\n",
       "     stroke  compression-ratio  horsepower  peak-rpm  city-mpg  highway-mpg  \n",
       "0 -1.839404          -0.288349    0.171065 -0.263484 -0.646553    -0.546059  \n",
       "1 -1.839404          -0.288349    0.171065 -0.263484 -0.646553    -0.546059  \n",
       "2  0.685920          -0.288349    1.261807 -0.263484 -0.953012    -0.691627  \n",
       "3  0.462157          -0.035973   -0.057230  0.787346 -0.186865    -0.109354  \n",
       "4  0.462157          -0.540725    0.272529  0.787346 -1.106241    -1.273900  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.loc[:, df.columns != 'price'].values\n",
    "y = df.loc[:, ['price']].values\n",
    "\n",
    "x = StandardScaler().fit_transform(x)\n",
    "x = pd.DataFrame(x, columns=list(features[:-1]))\n",
    "y = pd.DataFrame(y, columns=[features[-1]])\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing, its the time to divide the dataset into train and test. This has been done arbitrarily for this tutorial, but there exists different rules and information can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = x.as_matrix()[:190, :-1]\n",
    "Y_train = y.as_matrix()[:190, -1]\n",
    "X_test = x.as_matrix()[190:, :-1]\n",
    "Y_test = y.as_matrix()[190:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linear Regression with Scikit-learn \n",
    "In order to get a general idea of the result of Linear Regression, learn how to use a powerful library and later compare it with our algorithm, we are going to do first LR with the scikit-learn implementation.\n",
    "\n",
    "For a more complex algorithm, we can add the bias term, which is a parameter that does not depend on the input. After this, the output function is $\\hat{y} = b + w^\\top x$. Because we are implementing a vectorize version, we have to add a column of 1 to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_ = np.append(np.ones((X_train.shape[0], 1)), X_train, axis=1)\n",
    "Y_train_ = Y_train.reshape((Y_train.shape[0], 1))  #Y_train must be reshaped so it is a (nx1) vector\n",
    "X_test_ = np.append(np.ones((X_test.shape[0], 1)), X_test, axis=1)\n",
    "Y_test_ = Y_test.reshape((Y_test.shape[0], 1)) #Y_train must be reshaped so it is a (nx1) vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((190, 25), (190, 1), (15, 25), (190, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_.shape, Y_train_.shape, X_test_.shape, Y_train_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each algorithm in ML consists of the model itself, which here is the output function, and the cost function we try to minimize, which here is the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [[    0.           373.40605313  -446.7188729  -1471.8445747\n",
      "   -812.41938807   646.65597073  -271.94836005  -725.73579376\n",
      "    740.74704641  1572.796544     338.84808915   444.98696799\n",
      "   1273.0723887    726.46373081  1081.63274641   195.05573189\n",
      "    749.75684068  5740.0065274    572.48664847  -191.68185725\n",
      "  -1063.16873075  -925.14573751 -1892.40835535  1186.24467667\n",
      "    447.79619501]]\n",
      "Mean squared error: 10158646.65\n",
      "Variance score: 0.29\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model #the model itself\n",
    "from sklearn.metrics import mean_squared_error, r2_score #the cost function and variance score\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train_,Y_train_)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test_)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(Y_test_, y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(Y_test_, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Linear Regression implementation explained\n",
    "As previously explained, linear regression is a model that outputs a linear function of the given input. Formally, be the input vector $x \\in \\mathbb{R}^n$, the predicted value of a scalar $y \\in \\mathbb{R}$ is calculated by the formula $\\hat{y} = w^\\top x$ (vectorized form) or $\\hat{y} = w_1 x_1 + w_2 x_2 +...+ w_i x_i$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.ones((X_train_.shape[1], 1)) #theta (w) is created as a column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearRegression(X, theta):\n",
    "    return np.dot(X, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that the model is defined, we have to define the error metric we will try to minimize. \n",
    "\n",
    "In supervised training, we have the real output (the label), $y \\in \\mathbb{R}$, and the predicted output given by the model, $\\hat{y} \\in \\mathbb{R}$. The objective is that the predicted value is as similar as possible to the real output. For that reason, an error metric has to be defined to see how similar this two values are.\n",
    "\n",
    "Usually, the metric chosen is the mean squared error, defined by $MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2$.\n",
    "\n",
    "In case, anyone want to go in depth about why $MSE$ is used, here is a [link](https://stats.stackexchange.com/questions/16508/calculating-likelihood-from-rmse/16534)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error_scratch(Y_pred, Y):\n",
    "    return (1/(2*Y.shape[0]))*((Y_pred-Y).T@(Y_pred-Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pay attention, the above definition adds a term to the error function, now it is defined as $MSE = \\frac{1}{2n}\\sum_{i=1}^{n}(\\hat{y}_i-y_i)^2$. The reason is the following, after declaring the model and the error function, we have to come up with an algorithm that minimizes this error.\n",
    "\n",
    "There are two options, solve it analytically, like it is explained [here](https://towardsdatascience.com/big4-tech-interview-question-derive-the-linear-regression-c45ccdd213e3) or through gradient descent. In this tutorial, we we'll use the second option.\n",
    "\n",
    "#### What is gradient descent?\n",
    "As stated in the [Wikipedia](https://en.wikipedia.org/wiki/Gradient_descent), Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. This is the pseudocode of the Gradient Descent algorithm. ![Gradient Descent Pseudocode](https://cdn-images-1.medium.com/max/1040/1*oJKalifbWzwuo3fRjWJjTg.png)\n",
    "\n",
    "It consists of finding the minimum of the function by computing the gradient of the function, and updating the parameters in small steps until convergence. This image illustrates better the concept: ![Gradient Descent Example](https://cdn-images-1.medium.com/max/1600/1*f9a162GhpMbiTVTAua_lLQ.png)\n",
    "\n",
    "We'll modify it a little because instead of having a tolerance we will decide a fixed number of iterations of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameters: parameters of the Gradient Descent\n",
    "lr = 0.003 #(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, the gradient is required, but what is the gradient exactly?\n",
    "The gradient is, simply, the derivative of the error function (cost function) with respect to the parameter that we want to optimize, in this case theta.\n",
    "\n",
    "We will name the $MSE$ as the function (error) cost $J(\\theta)$ and the weights $w$ as the parameter $\\theta$. So the derivative is:\n",
    "$gradient = \\frac{\\partial J(\\theta)}{\\theta} = \\frac{\\partial}{\\partial \\theta} (\\frac{1}{2n}\\sum_{i=1}^{n}(\\hat{y}_i-y_i)^2) = \\frac{\\partial}{\\partial \\theta} (\\frac{1}{2n}\\sum_{i=1}^{n}(\\theta^\\top x_i-y_i)^2) = \\frac{1}{n}\\sum_{i=1}^{n}x_i(\\theta_i^\\top x_i-y_i))$. The vectorized formula, that we are going to use, is $gradient = \\frac{1}{n}x^\\top(\\theta^\\top x-y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient(X, theta, Y, lr):\n",
    "    h = linearRegression(X, theta)\n",
    "    derivative = X.T@(h - Y)\n",
    "    theta = theta - lr * derivative * 1/X.shape[0]\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all the theory behind Linear Regression, it wasn't so difficult, right? Now, let's see if it truly works. For that, we will implement the Gradient Descent pseudocode and plot the error, so as to see if it gets smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.62 s, sys: 15.4 ms, total: 3.64 s\n",
      "Wall time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_iter = 100000\n",
    "for i in range(n_iter):\n",
    "    Y_pred = linearRegression(X_train_, theta)\n",
    "    error.append(mean_squared_error_scratch(Y_pred, Y_train_).reshape((1))) #To adapt it for plotting\n",
    "    theta = gradient(X_train_, theta, Y_train_, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8b94b5d518>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFfBJREFUeJzt3X+MZWd93/H3Z3a8/okx9k4qx2tY\nQ52UFSKxOwUb2oYAIbYV2f8klVdpIamJ1bQkbUFNjIhI6qqqQtokQjE/XEppUGPHEJSskKmpwBVV\nG6jHcnD8g8WLneCNKR4wGIiL1+v99o977u7d8T33Xq9n9s4zfr+0o7nnOc/c8717Zj/z7HPPeSZV\nhSRpa1mYdwGSpPVnuEvSFmS4S9IWZLhL0hZkuEvSFmS4S9IWNNdwT/LhJI8muWeGvi9OcnuSu5Lc\nneSKE1GjJLVo3iP3jwCXzdj314Bbquoi4GrgfRtVlCS1bq7hXlWfAx4bbUvysiT/LcmdSf5nkr81\n7A6c2T1+IfDICSxVkpqyOO8CxrgR+CdV9UCSVzMYob8e+A3g00l+CTgdeOP8SpSkzW1ThXuSM4DX\nAB9LMmw+ufu8B/hIVf2HJJcCH03yiqo6PIdSJWlT21ThzmCa6NtV9aNj9l1DNz9fVX+a5BRgB/Do\nCaxPkpow7zdUj1FV3wEeSvIzABn4kW73V4E3dO0vB04BVudSqCRtcpnnqpBJbgJex2AE/nXg14HP\nAu8HzgVOAm6uquuT7Ab+I3AGgzdXf6WqPj2PuiVps5truEuSNsammpaRJK2PqW+oJvkw8FPAo1X1\nijH7fxb41W7ze8AvVtUXpz3vjh07ateuXc+uWkl6nrvzzju/UVVL0/rNcrXMR4DfA36/Z/9DwI9V\n1beSXM7gOvVXT3vSXbt2sbKyMsPhJUlDSf5yln5Tw72qPpdk14T9/3tk8/PAzlkOLEnaOOs9534N\n8Km+nUmuTbKSZGV11asYJWmjrFu4J/lxBuH+q319qurGqlququWlpalTRpKk47Qud6gmeSXwIeDy\nqvrmejynJOn4PeeRe5IXA58A/lFVffm5lyRJeq5muRTyyF2kSQ4wuIv0JICq+gDwbuAc4H3dYl+H\nqmp5owqWJE03y9Uye6bsfyvw1nWrSJL0nDV3h+qXv/5dfvvT+/jG956cdymStGk1F+4PfP17vPez\n+3nsrw/OuxRJ2rSaC/ch1zuTpH7NhfvwFzQVprsk9Wkv3OddgCQ1oLlwH3JaRpL6NRfuR6ZlDHdJ\n6tVcuDsxI0nTNRjuA76hKkn9mgt3p2Ukabr2wn3eBUhSA5oLd0nSdM2Fe+LYXZKmaS7ch5xzl6R+\nzYX7cNzu1TKS1K+9cHdWRpKmai7ch5yWkaR+zYX70VUhJUl92gt3r3SXpKmaC/ehcl5Gknq1F+5O\ny0jSVM2Fu5MykjRdc+E+5KyMJPVrLtyPLj9guktSn/bCfd4FSFIDmgv3IadlJKnf1HBP8uEkjya5\np2d/krw3yf4kdye5eP3LHD3eRj67JG0Ns4zcPwJcNmH/5cCF3ce1wPufe1nTOXCXpH5Tw72qPgc8\nNqHLVcDv18DngbOSnLteBa41vEPVaRlJ6rcec+7nAQ+PbB/o2p4hybVJVpKsrK6uHtfBnJaRpOnW\nI9zHxe3YcXVV3VhVy1W1vLS09JwO6vIDktRvPcL9AHD+yPZO4JF1eN6xvMpdkqZbj3DfC7y5u2rm\nEuDxqvraOjzveE7LSNJUi9M6JLkJeB2wI8kB4NeBkwCq6gPArcAVwH7gCeDnN6rYUc7KSFK/qeFe\nVXum7C/gn61bRVMcuVrGiRlJ6tXcHapeLSNJ0zUX7kc4cJekXs2FuwN3SZquuXAfcuAuSf2aC/fh\neu5eLSNJ/RoM93lXIEmbX3PhPuSlkJLUr7lwP7L8gNkuSb3aC3enZSRpqubCfciBuyT1azDch1fL\nGO+S1Ke5cHdaRpKmay7chxy3S1K/5sLdgbskTddcuB/h0F2SejUX7keWHzDdJalXe+E+7wIkqQHN\nhfuQV0JKUr/mwn14KaThLkn92gt3J2Ykaarmwn3Igbsk9Wsu3I9OyxjvktSnuXCXJE3XbLg7bpek\nfs2FuwuHSdJ0zYX7kFPuktRvpnBPclmSfUn2J7luzP4XJ7k9yV1J7k5yxfqX2h3r6C/a26hDSFLz\npoZ7km3ADcDlwG5gT5Lda7r9GnBLVV0EXA28b70LPVrPRj2zJG0ds4zcXwXsr6oHq+ogcDNw1Zo+\nBZzZPX4h8Mj6lTie0zKS1G9xhj7nAQ+PbB8AXr2mz28An07yS8DpwBvXpboxjlznvlEHkKQtYJaR\n+7iJkLXZugf4SFXtBK4APprkGc+d5NokK0lWVldXn321uPyAJM1ilnA/AJw/sr2TZ067XAPcAlBV\nfwqcAuxY+0RVdWNVLVfV8tLS0vFVfOS5ntOXS9KWNku43wFcmOSCJNsZvGG6d02frwJvAEjycgbh\nfnxD8ymOTsuY7pLUZ2q4V9Uh4G3AbcD9DK6KuTfJ9Umu7Lq9A/iFJF8EbgJ+rjZo8RcnZSRpulne\nUKWqbgVuXdP27pHH9wGvXd/SptV0Io8mSW1p7g5Vr5aRpOmaC3cnZiRpugbDfcD13CWpX3Ph7vID\nkjRdc+EuSZquuXA/siakszKS1Ku9cHdeRpKmai7ch7xDVZL6NRfuTstI0nTthbuzMpI0VXPhPuTI\nXZL6NRfuw/XczXZJ6tdeuDstI0lTNRfuQy4/IEn9mg13SVK/ZsPdcbsk9Wsu3I/MuZvuktSrwXD3\nHVVJmqa5cB9y+QFJ6tdcuLv8gCRN1164OysjSVM1F+5DDtwlqV9z4X5k+QHTXZJ6tRfuTstI0lTN\nhfuQV8tIUr/mwn04cndaRpL6NRfuCxnOuZvuktRnpnBPclmSfUn2J7mup88/SHJfknuT/MH6ljly\nnO7zYbNdknotTuuQZBtwA/ATwAHgjiR7q+q+kT4XAu8EXltV30ryAxtVsCN3SZpulpH7q4D9VfVg\nVR0EbgauWtPnF4AbqupbAFX16PqWedQw3B25S1K/WcL9PODhke0DXduoHwJ+KMn/SvL5JJeNe6Ik\n1yZZSbKyurp6fBV38zKHHblLUq9Zwn3cleVrk3URuBB4HbAH+FCSs57xRVU3VtVyVS0vLS0921oB\nWPA6d0maapZwPwCcP7K9E3hkTJ8/qaqnquohYB+DsF93R6dlHLlLUp9Zwv0O4MIkFyTZDlwN7F3T\n54+BHwdIsoPBNM2D61noUI5My2zEs0vS1jA13KvqEPA24DbgfuCWqro3yfVJruy63QZ8M8l9wO3A\nv6qqb25IwXFtGUmaZuqlkABVdStw65q2d488LuDt3ceGim+oStJUzd2henRVSMNdkvo0F+4Lri0j\nSVM1GO7exCRJ0zQX7s65S9J0DYZ7N+c+5zokaTNrLtxhMO/uG6qS1K/JcE/itIwkTdBkuA9G7vOu\nQpI2rybDfTByn3cVkrR5tRnuOOcuSZM0Ge4LiVfLSNIEjYY7HHZeRpJ6NRnuzrlL0mSNhjuUEzOS\n1KvJcF9IvBRSkiZoMtwT15aRpEmaDHdH7pI0WaPh7shdkiZpMtzBq2UkaZImw33w25hMd0nq02i4\nh8OH512FJG1eTYa7V8tI0mRNhrtry0jSZE2GuyN3SZqs2XA32yWpX5PhPriJyXSXpD4zhXuSy5Ls\nS7I/yXUT+v10kkqyvH4lPtOCq0JK0kRTwz3JNuAG4HJgN7Anye4x/V4A/DLwhfUu8hnHwjl3SZpk\nlpH7q4D9VfVgVR0EbgauGtPv3wDvAb6/jvWNNVjyV5LUZ5ZwPw94eGT7QNd2RJKLgPOr6pOTnijJ\ntUlWkqysrq4+62KHnHOXpMlmCfeMaTuSrEkWgN8B3jHtiarqxqparqrlpaWl2atcW1DwDlVJmmCW\ncD8AnD+yvRN4ZGT7BcArgP+R5C+AS4C9G/mm6uAmJkfuktRnlnC/A7gwyQVJtgNXA3uHO6vq8ara\nUVW7qmoX8Hngyqpa2ZCK8XeoStI0U8O9qg4BbwNuA+4Hbqmqe5Ncn+TKjS5wnAzqmsehJakJi7N0\nqqpbgVvXtL27p+/rnntZky0seIeqJE3S7B2qXucuSf2aDPfBTUzzrkKSNq82w90lfyVpoibDfSG+\noSpJkzQZ7nHOXZImajLcF7xDVZImajTcw9OO3CWpV5PhvrgtHPZyGUnq1WS4LyQcMtwlqVeT4b64\n4BuqkjRJk+G+bSEcetpwl6Q+zYa7I3dJ6tdsuDvnLkn9Gg33Ba+WkaQJ2gz34MhdkiZoM9wXFnja\ncJekXk2G++JCDHdJmqDJcF/wDVVJmqjJcPcmJkmarMlwH9zE5LKQktSn2XB3VkaS+jUb7odc0F2S\nejUb7ma7JPVrMtwXHblL0kRNhvtCBnPu/pJsSRqvyXBfXAiANzJJUo+Zwj3JZUn2Jdmf5Lox+9+e\n5L4kdyf5TJKXrH+pRy104e6NTJI03tRwT7INuAG4HNgN7Emye023u4Dlqnol8HHgPetd6KjhyN0b\nmSRpvFlG7q8C9lfVg1V1ELgZuGq0Q1XdXlVPdJufB3aub5nH2ubIXZImmiXczwMeHtk+0LX1uQb4\n1LgdSa5NspJkZXV1dfYq1xiGu2u6S9J4s4R7xrSNTdUk/xBYBn5r3P6qurGqlqtqeWlpafYq13Dk\nLkmTLc7Q5wBw/sj2TuCRtZ2SvBF4F/BjVfXk+pQ33javlpGkiWYZud8BXJjkgiTbgauBvaMdklwE\nfBC4sqoeXf8yj7XoyF2SJpoa7lV1CHgbcBtwP3BLVd2b5PokV3bdfgs4A/hYkj9Lsrfn6dbF9sVB\n2QcPeZeqJI0zy7QMVXUrcOuatnePPH7jOtc10cmL2wB48tDTJ/KwktSMJu9QPdmRuyRN1GS4D6dl\nnjTcJWmsJsN9OC3jyF2Sxmsy3I+O3J1zl6Rxmgz34Zz7k085cpekcZoM9yOXQvpLsiVprCbD3ZG7\nJE3WaLh317k7cpeksZoM9yNvqD7lG6qSNE6T4X6y17lL0kTNhvtC4ImDh+ZdiiRtSk2GexLOPPUk\nvvt9w12Sxmky3AHOPOUkvvP/npp3GZK0KbUb7qcu8h1H7pI0VrPh/sJTHblLUp9mw/3MU07iO983\n3CVpnGbD/ezTt/Podzf0V7VKUrOaDfcXn30a337iKUfvkjRGs+F+/tmnAfDVbz4x50okafNpNtxf\nfu6ZANz18LfnXIkkbT7Nhvuuc07jJeecxsdWHub7rjEjScdYnHcBxysJ73jTD/PLN93Fpf/uM7xy\n51mc96JT2XHGyZxz+nZOP3mR07Zv49Tt2zjtpG2ctn2R7YsLLG4LiwthcdvC4PPo421hcWGwtEGS\neb9ESTpuzYY7wJU/8oPsOGM7H7/zAF/62ne5568e57EnDlK1Ps+fwEJCGHxm8GfQNrIv3Q+DY9uG\n2xAGj8f9uJjlh8i4LmPbxhxhfL/Z6hhbWab38QejNNnVf+d83vr3Xrqhx2g63AFe87IdvOZlO45s\nP324+NYTB3niyad54qlDPHHw6cHjg4d46uni0OHDHBp+Plzd4+LQ00e3D1dRQFVRxZHtw1Uw+MPh\nw0fbqrq+cEz/Y75+zA+ccT+D1varcb1mfq5nts5yzFmfb+zP0HX6wSptZTvOOHnDj9F8uK+1bSGD\nv7gz5l2JJM1Ps2+oSpL6zRTuSS5Lsi/J/iTXjdl/cpI/7PZ/Icmu9S5UkjS7qeGeZBtwA3A5sBvY\nk2T3mm7XAN+qqr8J/A7wm+tdqCRpdrOM3F8F7K+qB6vqIHAzcNWaPlcB/6V7/HHgDfGSCUmam1nC\n/Tzg4ZHtA13b2D5VdQh4HDhn7RMluTbJSpKV1dXV46tYkjTVLOE+bgS+9oK3WfpQVTdW1XJVLS8t\nLc1SnyTpOMwS7geA80e2dwKP9PVJsgi8EHhsPQqUJD17s4T7HcCFSS5Ish24Gti7ps9e4C3d458G\nPlvj7qCRJJ0QmSWDk1wB/C6wDfhwVf3bJNcDK1W1N8kpwEeBixiM2K+uqgenPOcq8JfHWfcO4BvH\n+bWt8jU/P/ianx+ey2t+SVVNndeeKdw3myQrVbU87zpOJF/z84Ov+fnhRLxm71CVpC3IcJekLajV\ncL9x3gXMga/5+cHX/Pyw4a+5yTl3SdJkrY7cJUkTGO6StAU1F+7Tlh/ezJKcn+T2JPcnuTfJP+/a\nz07y35M80H1+UdeeJO/tXuvdSS4eea63dP0fSPKWkfa/neTPu69572ZZwC3JtiR3Jflkt31Btzz0\nA91y0du79t7lo5O8s2vfl+QnR9o33fdEkrOSfDzJl7rzfelWP89J/mX3fX1PkpuSnLLVznOSDyd5\nNMk9I20bfl77jjHR4FfBtfHB4CaqrwAvBbYDXwR2z7uuZ1H/ucDF3eMXAF9msIzye4DruvbrgN/s\nHl8BfIrB2j2XAF/o2s8GHuw+v6h7/KJu3/8BLu2+5lPA5fN+3V1dbwf+APhkt30Lg5vdAD4A/GL3\n+J8CH+geXw38Yfd4d3e+TwYu6L4Ptm3W7wkGq6S+tXu8HThrK59nBosHPgScOnJ+f26rnWfg7wMX\nA/eMtG34ee07xsRa5/2P4Fn+xV4K3Day/U7gnfOu6zm8nj8BfgLYB5zbtZ0L7OsefxDYM9J/X7d/\nD/DBkfYPdm3nAl8aaT+m3xxf507gM8DrgU9237jfABbXnlfgNuDS7vFi1y9rz/Ww32b8ngDO7IIu\na9q37Hnm6MqwZ3fn7ZPAT27F8wzs4thw3/Dz2neMSR+tTcvMsvxwE7r/hl4EfAH4G1X1NYDu8w90\n3fpe76T2A2Pa5+13gV8BDnfb5wDfrsHy0HBsnX3LRz/bv4t5eimwCvznbirqQ0lOZwuf56r6K+Df\nA18FvsbgvN3J1j7PQyfivPYdo1dr4T7T0sKbXZIzgD8C/kVVfWdS1zFtdRztc5Pkp4BHq+rO0eYx\nXWvKvmZeM4OR6MXA+6vqIuCvGfxXuk/zr7mbA76KwVTKDwKnM/jtbWttpfM8zVxfY2vhPsvyw5ta\nkpMYBPt/rapPdM1fT3Jut/9c4NGuve/1TmrfOaZ9nl4LXJnkLxj8Fq/XMxjJn5XB8tBwbJ19y0c/\n27+LeToAHKiqL3TbH2cQ9lv5PL8ReKiqVqvqKeATwGvY2ud56ESc175j9Got3GdZfnjT6t75/k/A\n/VX12yO7RpdMfguDufhh+5u7d90vAR7v/kt2G/CmJC/qRkxvYjAf+TXgu0ku6Y715pHnmouqemdV\n7ayqXQzO12er6meB2xksDw3PfM3jlo/eC1zdXWVxAXAhgzefNt33RFX9X+DhJD/cNb0BuI8tfJ4Z\nTMdckuS0rqbha96y53nEiTivfcfoN883YY7zzYwrGFxl8hXgXfOu51nW/ncZ/DfrbuDPuo8rGMw1\nfgZ4oPt8dtc/DH45+VeAPweWR57rHwP7u4+fH2lfBu7pvub3WPOm3pxf/+s4erXMSxn8o90PfAw4\nuWs/pdve3+1/6cjXv6t7XfsYuTpkM35PAD8KrHTn+o8ZXBWxpc8z8K+BL3V1fZTBFS9b6jwDNzF4\nT+EpBiPta07Eee07xqQPlx+QpC2otWkZSdIMDHdJ2oIMd0naggx3SdqCDHdJ2oIMd0naggx3SdqC\n/j+gRPQnmTcyywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8ba3f68fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we can compare it with the scikit-learn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 10111918.17\n",
      "Mean squared error from scratch: 5055959.09\n",
      "Variance score: 0.29\n",
      "Variance score from scratch: 0.29\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the testing set\n",
    "y_pred_mine = linearRegression(X_test_, theta)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(Y_test_, y_pred_mine))\n",
    "print(\"Mean squared error from scratch: %.2f\"\n",
    "      % mean_squared_error_scratch(Y_test_, y_pred_mine))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(Y_test, y_pred))\n",
    "print('Variance score from scratch: %.2f' % r2_score(Y_test, y_pred_mine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the only difference is that our mean squared error is the half of the scikit-learn, this is due to the constant that we added to our function, the $\\frac{1}{2}$, to facilitate the derivative of the function.\n",
    "\n",
    "### 3.1 Adding regularization\n",
    "If we print the parameter $\\theta$ of the model, we'll see that it takes incredibly high numbers. This is called overfitting, which means that the model has learned the training dataset TOO good. To fix this, we are going to add to our function cost the parameter itself, so the bigger the values of $\\theta$, the greater the error. Now, the error function is: $J(\\theta) = \\frac{1}{2n}(\\sum_{i=1}^{n}(\\hat{y}_i-y_i)^2 + \\lambda \\sum_{j=1}^{n}\\theta_j^2)$ and the $gradient = \\frac{\\partial J(\\theta)}{\\theta} = \\frac{1}{n}(\\sum_{i=1}^{n}x_i(\\theta_i^\\top x_i-y_i) + \\lambda \\sum_{i=1}^{n} \\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error_reg(Y_pred, Y, theta, lam):\n",
    "    return (1/(2*Y.shape[0]))*(((Y_pred-Y).T@(Y_pred-Y))+lam*theta.T@theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_reg(X, theta, Y, lr, lam):\n",
    "    h = linearRegression(X, theta)\n",
    "    derivative = X.T@(h - Y)+lam*theta\n",
    "    theta = theta - lr * derivative * 1/X.shape[0]\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lam = 1\n",
    "error_reg = []\n",
    "theta_reg = np.ones((X_train_.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.87 s, sys: 7.76 ms, total: 4.88 s\n",
      "Wall time: 2.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_iter = 100000\n",
    "for i in range(n_iter):\n",
    "    Y_pred = linearRegression(X_train_, theta_reg)\n",
    "    error_reg.append(mean_squared_error_reg(Y_pred, Y_train_, theta_reg, lam).reshape((1))) #To adapt it for plotting\n",
    "    theta_reg = gradient_reg(X_train_, theta_reg, Y_train_, lr, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8b9398e160>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b94ac2eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(error_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 10158646.65\n",
      "Mean squared error from scratch: 10111918.17\n",
      "Mean squared error from scratch: 9991694.04\n",
      "Variance score: 0.29\n",
      "Variance score from scratch: 0.29\n",
      "Variance score from scratch reg: 0.30\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the testing set\n",
    "y_pred_mine = linearRegression(X_test_, theta)\n",
    "y_pred_mine_reg = linearRegression(X_test_, theta_reg)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(Y_test_, y_pred))\n",
    "print(\"Mean squared error from scratch: %.2f\" % mean_squared_error(Y_test_, y_pred_mine))\n",
    "print(\"Mean squared error from scratch: %.2f\" % mean_squared_error(Y_test_, y_pred_mine_reg))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(Y_test, y_pred))\n",
    "print('Variance score from scratch: %.2f' % r2_score(Y_test, y_pred_mine))\n",
    "print('Variance score from scratch reg: %.2f' % r2_score(Y_test, y_pred_mine_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compare the performance of the models, we can see that the one that performs the better is the regularized, the last one that we have implemented.\n",
    "\n",
    "Reference:\n",
    "1. https://www.cs.utah.edu/~piyush/teaching/6-9-slides.pdf\n",
    "2. https://stats.stackexchange.com/questions/87188/optimization-of-the-regularized-least-squares-with-gradient-descent\n",
    "3. http://theory.stanford.edu/~tim/s16/l/l6.pdf\n",
    "4. https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/\n",
    "5. http://www.hongliangjie.com/notes/lr.pdf\n",
    "6. https://arxiv.org/pdf/1712.08597.pdf\n",
    "7. https://www.coursera.org/lecture/ml-regression/computing-the-gradient-of-the-ridge-objective-kvaqc\n",
    "8. https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a\n",
    "10. [Deep Learning Book by Ian Goodfellow and Yoshua Bengio and Aaron Courville](https://www.deeplearningbook.org/)\n",
    "11. [Coursera - Machine Learning](https://www.coursera.org/learn/machine-learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
